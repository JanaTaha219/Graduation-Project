{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42f8a8f",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8d4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\HP\n",
      "[nltk_data]     User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the pre-trained Word2Vec model\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e66b2",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdeef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55c032",
   "metadata": {},
   "source": [
    "## stemming \n",
    "<br>\n",
    "stemming:-Stemming: remove prefix and suffix <br>\n",
    "-e.g.: Original Word: \"running\" <br>\n",
    "Stem: \"run\" <br>\n",
    "Stemming is not accurate, may create non-words, but it's fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a54a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    text = ' '.join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852e312",
   "metadata": {},
   "source": [
    "## lemmatization\n",
    "<br>\n",
    "Lemmatization: reducing words to their base or dictionary form. <br>\n",
    "-e.g.: Original Word: \"better\" <br>\n",
    "Lemma: \"good\" <br>\n",
    "lemmatization is more accurate, returns real words but it's slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d297608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afa138",
   "metadata": {},
   "source": [
    "## w2v_model\n",
    "<br>\n",
    "numerical representations of words in a high-dimensional vector space where words with similar meanings are closer to each other.\n",
    "<br>\n",
    "- Word: King (vector: [0.8, 0.5, 0.3]) <br>\n",
    "- Word: Queen (vector: [0.75, 0.45, 0.25]) (Notice the closeness) <br>\n",
    "- Word: Dog (vector: [0.2, 0.7, 0.1]) (Further away due to different meaning) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acad19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vector(text):\n",
    "    word_embeddings = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            embedding = w2v_model[word]\n",
    "            word_embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            # If the word is not found in the vocabulary, skip it\n",
    "            pass\n",
    "    \n",
    "    if len(word_embeddings) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calculate the average of word embeddings\n",
    "    text_vector = np.mean(word_embeddings, axis=0)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bdbe6",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "<br>\n",
    "ما بعرف شو هي :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deef2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_between_texts(text1, text2, limm=True):\n",
    "    # Preprocess the texts\n",
    "    if limm == True:\n",
    "        preprocessed_text1 = apply_lemmatization(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_lemmatization(preprocess_text(text2))\n",
    "    else:\n",
    "        preprocessed_text1 = apply_stemming(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_stemming(preprocess_text(text2))\n",
    "    \n",
    "    # Get text vectors\n",
    "    vector1 = get_text_vector(preprocessed_text1)\n",
    "    vector2 = get_text_vector(preprocessed_text2)\n",
    "    \n",
    "    if vector1 is None or vector2 is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b22458",
   "metadata": {},
   "source": [
    "### Trying consine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba7c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between Text 1 and Text 2 with lemmatization: 0.49738303\n",
      "Cosine Similarity between Text 1 and Text 2 with stemming: 0.46470538\n"
     ]
    }
   ],
   "source": [
    "# Example texts\n",
    "text1 = \"Tokenization is an important step in natural language processing.\"\n",
    "text2 = \"Sentence tokenization is a technique used to split text into sentences.\"\n",
    "\n",
    "# Calculate cosine similarity between texts\n",
    "similarity = cosine_similarity_between_texts(text1, text2)\n",
    "\n",
    "print(\"Cosine Similarity between Text 1 and Text 2 with lemmatization:\", similarity)\n",
    "similarity = cosine_similarity_between_texts(text1, text2, False)\n",
    "print(\"Cosine Similarity between Text 1 and Text 2 with stemming:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e79a7",
   "metadata": {},
   "source": [
    "## Connecting to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d040ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "db = mysql.connector.connect(user='root', password='Jana2003?',\n",
    "                              host='127.0.0.1', database='grad',\n",
    "                              auth_plugin='mysql_native_password')\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7055d",
   "metadata": {},
   "source": [
    "## عشان نجيب كل النوتس الي بالداتابيس ما عدا النوتس تبعت اليوزر يلي بدنا نعمله ريكومند + يلي عاملهم فلو \n",
    "\n",
    "## To retrieve all notes except those belonging to the user for whom we wish to recommend posts and those who does follow them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "076f63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_users_note_except_user_and_followed(username_to_exclude):\n",
    "    # Fetch the list of users that the specified user follows\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT following_id\n",
    "        FROM followers\n",
    "        WHERE follower_id = '{username_to_exclude}'\n",
    "    \"\"\")\n",
    "    followed_users = cursor.fetchall()\n",
    "    \n",
    "    # Extract the list of followed user IDs\n",
    "    followed_user_ids = [user[0] for user in followed_users]\n",
    "    \n",
    "    # Convert the list to a string format suitable for SQL IN clause\n",
    "    followed_user_ids_str = \"', '\".join(followed_user_ids)\n",
    "    \n",
    "    # Include the specified user in the exclusion list\n",
    "    followed_user_ids_str = f\"'{username_to_exclude}', '{followed_user_ids_str}'\"\n",
    "    \n",
    "    # Fetch notes excluding those from the specified user and followed users\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM note\n",
    "        WHERE user_id NOT IN ({followed_user_ids_str})\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    notes = cursor.fetchall()\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed565f1f",
   "metadata": {},
   "source": [
    "### testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1958a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa\n",
      "a\n",
      "aaaaa\n",
      "saraa\n",
      "saraa\n",
      "saraa\n",
      "saraa\n",
      "saraa\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "all_users_post_except_specific_user = get_all_users_note_except_user_and_followed(\"aaa\")\n",
    "for post in all_users_post_except_specific_user:\n",
    "    print(post[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d639a30",
   "metadata": {},
   "source": [
    "## عشان نجيب النوتس تبعت اليوزر يلي بدنا نعمله ريكومند\n",
    "## To fetch all the user's notes from the database in order to recommend posts for them based on those notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffe8c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an updated text3!   this is a text from a url\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2628655"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_user_posts(username):\n",
    "    cursor.execute(f\"SELECT text FROM note WHERE user_id = '{username}'\")\n",
    "    posts = cursor.fetchall()\n",
    "    # Convert each tuple to a string\n",
    "    posts_as_strings = [post[0] for post in posts]\n",
    "    return posts_as_strings\n",
    "\n",
    "posts = get_all_user_posts(\"aaa\")\n",
    "print(posts[0], \" \", posts[1])\n",
    "cosine_similarity_between_texts(posts[0], posts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da098d",
   "metadata": {},
   "source": [
    "## Compine all previous method to recommend user notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6c59674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_user_posts_using_cosine(user_name, number_of_posts):\n",
    "    user_posts = get_all_user_posts(user_name)\n",
    "\n",
    "    # Get all other users' posts\n",
    "    other_users_posts = get_all_users_note_except_user_and_followed(user_name)\n",
    "\n",
    "    # Initialize recommendations list\n",
    "    recommendations = []\n",
    "\n",
    "    # Initialize a set to store the IDs of processed posts\n",
    "    processed_post_ids = set()\n",
    "\n",
    "    # Loop through each other user's post\n",
    "    for post in other_users_posts:\n",
    "        # Calculate cosine similarity between each user post and the current post\n",
    "        for user_post in user_posts:\n",
    "            similarity = cosine_similarity_between_texts(user_post, post[3])\n",
    "            if similarity and post[0] not in processed_post_ids:  # Check if similarity is not None and post not processed\n",
    "                recommendations.append({\n",
    "                    'id': post[0],              # Get the post ID from the tuple\n",
    "                    'creation_time': post[1],   # Get the creation time from the tuple\n",
    "                    'user_id': post[2],         # Get the user ID from the tuple\n",
    "                    'text': post[3],            # Get the post text from the tuple\n",
    "                    'url': post[4],             # Get the URL from the tuple\n",
    "                    'comment': post[5],         # Get the comment from the tuple\n",
    "                    'similarity': similarity    # Include the calculated similarity\n",
    "                })\n",
    "                # Add the post ID to the set of processed post IDs\n",
    "                processed_post_ids.add(post[0])\n",
    "\n",
    "    # Sort recommendations by similarity (highest first)\n",
    "    recommendations.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    # Return only the requested number of recommendations\n",
    "    return recommendations[:number_of_posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f10138",
   "metadata": {},
   "source": [
    "### Testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f34da06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'creation_time': datetime.datetime(2024, 4, 13, 18, 13, 29, 89000),\n",
       "  'user_id': 'aaaa',\n",
       "  'text': 'this is an updated text2!',\n",
       "  'url': 'www.hello.com',\n",
       "  'comment': \"this is jaaa's comment\",\n",
       "  'similarity': 1.0},\n",
       " {'id': 4,\n",
       "  'creation_time': datetime.datetime(2024, 4, 13, 18, 23, 35, 291000),\n",
       "  'user_id': 'a',\n",
       "  'text': 'this is a text from a url',\n",
       "  'url': 'www.hello.com',\n",
       "  'comment': \"this is a's comment\",\n",
       "  'similarity': 0.2628655},\n",
       " {'id': 6,\n",
       "  'creation_time': datetime.datetime(2024, 4, 16, 20, 55, 6, 597000),\n",
       "  'user_id': 'saraa',\n",
       "  'text': 'this is a text from a url',\n",
       "  'url': 'www.hello.com',\n",
       "  'comment': \"this is a's comment\",\n",
       "  'similarity': 0.2628655},\n",
       " {'id': 7,\n",
       "  'creation_time': datetime.datetime(2024, 4, 16, 20, 55, 8, 120000),\n",
       "  'user_id': 'saraa',\n",
       "  'text': 'this is a text from a url',\n",
       "  'url': 'www.hello.com',\n",
       "  'comment': \"this is a's comment\",\n",
       "  'similarity': 0.2628655},\n",
       " {'id': 8,\n",
       "  'creation_time': datetime.datetime(2024, 4, 16, 20, 55, 9, 448000),\n",
       "  'user_id': 'saraa',\n",
       "  'text': 'this is a text from a url',\n",
       "  'url': 'www.hello.com',\n",
       "  'comment': \"this is a's comment\",\n",
       "  'similarity': 0.2628655}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_user_posts_using_cosine(\"aaa\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83474f1",
   "metadata": {},
   "source": [
    "## Euclidean similarity\n",
    "<br>\n",
    "ما بعرف شو هي :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c1df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def euclidean_similarity_between_texts(text1, text2, limm=True):\n",
    "    # Preprocess the texts\n",
    "    if limm == True:\n",
    "        preprocessed_text1 = apply_lemmatization(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_lemmatization(preprocess_text(text2))\n",
    "    else:\n",
    "        preprocessed_text1 = apply_stemming(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_stemming(preprocess_text(text2))\n",
    "    \n",
    "    # Get text vectors\n",
    "    vector1 = get_text_vector(preprocessed_text1)\n",
    "    vector2 = get_text_vector(preprocessed_text2)\n",
    "    \n",
    "    if vector1 is None or vector2 is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate Euclidean distance\n",
    "    distance = euclidean_distances([vector1], [vector2])[0][0]\n",
    "    \n",
    "    # Convert distance to similarity\n",
    "    similarity = 1 / (1 + distance)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a0afa",
   "metadata": {},
   "source": [
    "## Compine all previous method to recommend user notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68e0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_user_posts_using_euclidean_distances(user_name, number_of_posts):\n",
    "    \n",
    "    user_posts = get_all_user_posts(user_name)\n",
    "\n",
    "    # Get all other users' posts\n",
    "    other_users_posts = get_all_users_note_except_user(user_name)\n",
    "\n",
    "    # Initialize recommendations list\n",
    "    recommendations = []\n",
    "\n",
    "    # Loop through each other user's post\n",
    "    for post in other_users_posts:\n",
    "    # Calculate cosine similarity between each user post and the current post\n",
    "        for user_post in user_posts:\n",
    "            similarity = euclidean_similarity_between_texts(user_post, post[0])\n",
    "            if similarity:  # Check if similarity is not None\n",
    "                recommendations.append({\n",
    "                  'id': post[1],  # Get the post ID from the tuple\n",
    "                  'text': post[0],  # Get the post text from the tuple\n",
    "                  'similarity': similarity\n",
    "                })\n",
    "\n",
    "    # Sort recommendations by similarity (highest first)\n",
    "    recommendations.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    # Return only the requested number of recommendations\n",
    "    return recommendations[:number_of_posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e8741",
   "metadata": {},
   "source": [
    "### Testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04a2152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 12, 'text': 'this is an updated text3!', 'similarity': 1.0},\n",
       " {'id': 15, 'text': 'Google Chrome', 'similarity': 0.22105131955806642},\n",
       " {'id': 4,\n",
       "  'text': 'this is a text from a url',\n",
       "  'similarity': 0.2133374349746918},\n",
       " {'id': 6,\n",
       "  'text': 'this is a text from a url',\n",
       "  'similarity': 0.2133374349746918},\n",
       " {'id': 7,\n",
       "  'text': 'this is a text from a url',\n",
       "  'similarity': 0.2133374349746918}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_user_posts_using_euclidean_distances(\"aaaa\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f6055",
   "metadata": {},
   "source": [
    "# Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539e2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_similarity_between_texts(text1, text2, limm=True):\n",
    "    # Preprocess the texts\n",
    "    if limm == True:\n",
    "        preprocessed_text1 = apply_lemmatization(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_lemmatization(preprocess_text(text2))\n",
    "    else:\n",
    "        preprocessed_text1 = apply_stemming(preprocess_text(text1))\n",
    "        preprocessed_text2 = apply_stemming(preprocess_text(text2))\n",
    "    \n",
    "    # Get text vectors\n",
    "    vector1 = get_text_vector(preprocessed_text1)\n",
    "    vector2 = get_text_vector(preprocessed_text2)\n",
    "    \n",
    "    if vector1 is None or vector2 is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Normalize vectors\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e391ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_user_posts_using_dot_product(user_name, number_of_posts):\n",
    "    \n",
    "    user_posts = get_all_user_posts(user_name)\n",
    "\n",
    "    # Get all other users' posts\n",
    "    other_users_posts = get_all_users_note_except_user(user_name)\n",
    "\n",
    "    # Initialize recommendations list\n",
    "    recommendations = []\n",
    "\n",
    "    # Loop through each other user's post\n",
    "    for post in other_users_posts:\n",
    "    # Calculate cosine similarity between each user post and the current post\n",
    "        for user_post in user_posts:\n",
    "            similarity = dot_product_similarity_between_texts(user_post, post[0])\n",
    "            if similarity:  # Check if similarity is not None\n",
    "                recommendations.append({\n",
    "                  'id': post[1],  # Get the post ID from the tuple\n",
    "                  'text': post[0],  # Get the post text from the tuple\n",
    "                  'similarity': similarity\n",
    "                })\n",
    "\n",
    "    # Sort recommendations by similarity (highest first)\n",
    "    recommendations.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    # Return only the requested number of recommendations\n",
    "    return recommendations[:number_of_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dcdc738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 12, 'text': 'this is an updated text3!', 'similarity': 1.0000001},\n",
       " {'id': 4, 'text': 'this is a text from a url', 'similarity': 0.2628655},\n",
       " {'id': 6, 'text': 'this is a text from a url', 'similarity': 0.2628655},\n",
       " {'id': 7, 'text': 'this is a text from a url', 'similarity': 0.2628655},\n",
       " {'id': 8, 'text': 'this is a text from a url', 'similarity': 0.2628655}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_user_posts_using_dot_product(\"aaaa\",5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
